{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Atlantic Scraper\n",
    "\n",
    "This notebook contains a scraper for online news articles from the Atlantic. It scrapes articles from the newest to oldest in a given page range, omiting any previously scraped articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle as pkl\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(my_article):\n",
    "    \"\"\"Return date from article in yyyy-mm-dd format or yyyy-mm, unless it is from an issue\"\"\"\n",
    "    article_date = '?'\n",
    "    if my_article.find('time').text is not None:        \n",
    "        soup_date = my_article.find('time').text\n",
    "        if 'ET' in soup_date:\n",
    "            article_date = date.today()\n",
    "        else:\n",
    "            date_string = soup_date.replace('\\n', '').strip()\n",
    "            article_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "    \n",
    "    return article_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(my_soup):\n",
    "    \"\"\"Returns the body text of a web page from the page soup\"\"\"\n",
    "    body_text = ''\n",
    "    \n",
    "    i = 0\n",
    "    while True: \n",
    "        paragraph = my_soup.find('section', {'id': 'article-section-' + str(i)})\n",
    "        if paragraph is not None:\n",
    "            body_text += paragraph.text\n",
    "            i += 1\n",
    "        elif i==0:\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return body_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(my_soup):\n",
    "    \"Return the article type for an article webpage in Atlantic\"\n",
    "    if my_soup.find('a', {'class': 'c-rubric__link'}) is not None:\n",
    "        category = my_soup.find('a', {'class': 'c-rubric__link'}).text.strip()\n",
    "    elif my_soup.find('a', {'class': 'rubric'}) is not None:\n",
    "        category = my_soup.find('a', {'class': 'rubric'}).text.strip()\n",
    "    else:\n",
    "        category = '?'\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(my_date):\n",
    "    \"\"\"Generate a file name for a particular date that is in datetime format\"\"\"\n",
    "    date_str = datetime.strftime(my_date, '%B %d, %Y')\n",
    "    date_list = date_str.split()\n",
    "    file_name = '{}_{}_{}'.format(date_list[0], date_list[1], date_list[2])\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the functions above to scrape the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'date': [], 'category': [], 'title': [], 'subtitle': [],\n",
    "             'author': [], 'author_bio': [], 'text': [],  'url': [], 'time_scraped': []}    \n",
    "        \n",
    "for i in range(1, 100):    \n",
    "    page_url = 'https://www.theatlantic.com/latest/?page=' + str(i) \n",
    "    #a page of \"Latest\": links to articles\n",
    "    page = requests.get(page_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all('li', {'class': 'article blog-article'})\n",
    "             \n",
    "    for article in articles:\n",
    "        \n",
    "        article_date = get_date(article)\n",
    "        filename = file_name(article_date)\n",
    "        \n",
    "        filepath = 'data/raw/{}.pkl'.format(filename)\n",
    "        \n",
    "        #check if file exists already\n",
    "        if os.path.isfile(filepath):\n",
    "            continue          \n",
    "        print('scraping from page {}'.format(i)) \n",
    "        \n",
    "        #check if date is same or is first entry\n",
    "        if len(data_dict['date']) > 0 and article_date != data_dict['date'][-1]:\n",
    "            pkl_name = file_name(data_dict['date'][-1])\n",
    "            pkl_filepath = 'data/raw/{}.pkl'.format(pkl_name)\n",
    "                      \n",
    "            with open(pkl_filepath, 'wb') as fp:\n",
    "                pkl.dump(data_dict, fp)\n",
    "                          \n",
    "            data_dict = {'date': [], 'category': [], 'title': [], 'subtitle': [],\n",
    "             'author': [], 'author_bio': [], 'text': [],  'url': [], 'time_scraped': []}\n",
    "            \n",
    "        #scraping\n",
    "        \n",
    "        data_dict['date'].append(article_date)\n",
    "\n",
    "        a_element = article.find('a')\n",
    "        url = 'https://www.theatlantic.com{}'.format(a_element.get('href'))\n",
    "        data_dict['url'].append(url)  \n",
    "        \n",
    "        if article.find('li', {'class': 'byline'}) is not None:\n",
    "            author = article.find('li', {'class': 'byline'}).text\n",
    "        else:\n",
    "            author = '?'\n",
    "        data_dict['author'].append(author)\n",
    "        \n",
    "        if article.find('h2') is not None:\n",
    "            title = article.find('h2').text.replace('\\n', '').strip()\n",
    "        else:\n",
    "            title = '?'\n",
    "        data_dict['title'].append(title)       \n",
    " \n",
    "        if article.find('p', {'class':'dek has-dek'}) is not None: \n",
    "            subtitle = article.find('p', {'class':'dek has-dek'}).text.replace('\\n', '').replace('  ', '')\n",
    "        else:\n",
    "            subtitle = '?'\n",
    "        data_dict['subtitle'].append(subtitle)\n",
    "        \n",
    "        time.sleep(np.random.random() + 3)\n",
    "        \n",
    "        #going to the page for the article itself        \n",
    "        artic_page = requests.get(url) \n",
    "        artic_soup = BeautifulSoup(artic_page.content, 'html.parser')\n",
    "       \n",
    "        category = get_category(artic_soup) \n",
    "        data_dict['category'].append(category)        \n",
    "              \n",
    "        body = get_body(artic_soup) \n",
    "        data_dict['text'].append(body)\n",
    "        \n",
    "        if artic_soup.find('div', {'class': 'c-article-writer__bio'}) is not None:\n",
    "            author_bio = artic_soup.find('div', {'class': 'c-article-writer__bio'}).text.strip()\n",
    "        else:\n",
    "            author_bio = '?'\n",
    "        data_dict['author_bio'].append(author_bio)\n",
    "        \n",
    "        scrape_time = datetime.now()\n",
    "        data_dict['time_scraped'].append(scrape_time)\n",
    "                          \n",
    "        time.sleep(np.random.random() + 3)\n",
    "    \n",
    "    time.sleep(np.random.random() + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data scraped using this notebook will be used and anlyzed in the following notebooks in this repo: 2.0_atlantic_eda_cleaning.ipynb, 3.0_atlantic_sentiment_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
